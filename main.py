import streamlit as st
from langchain_core.messages import ChatMessage
from agent import modelCreation
from openai import OpenAI
import pygame
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
import tempfile
from transformers import pipeline
from audiorecorder import audiorecorder




st.set_page_config(page_title="Ready2Interview", page_icon="ğŸ’¬")
transcriber = pipeline("automatic-speech-recognition", model="openai/whisper-small")
emotion_recognizer = pipeline(model="superb/wav2vec2-base-superb-er", task="audio-classification")


if "messages" not in st.session_state:
    st.session_state["messages"] = []

if "store" not in st.session_state:
    st.session_state["store"] = dict()



audio = audiorecorder("ğŸ¤", "ğŸ›‘")

if len(audio) > 0: 

    # To save audio to a file, use pydub export method:
    audio.export("audio.wav", format="wav")

def print_history():
    for msg in st.session_state["messages"]:
        st.chat_message(msg.role).write(msg.content)


def add_history(role, content):
    st.session_state["messages"].append(ChatMessage(role=role, content=content))

def read_outloud(stream_response):
    client = OpenAI()

    # Create a temporary file for speech.mp3
    with tempfile.NamedTemporaryFile(delete=False, suffix=".mp3") as temp_file:
        speech_file_path = temp_file.name

    response = client.audio.speech.create(
        model="tts-1",
        voice="shimmer",
        input=stream_response
    )

    # Stream the response directly to the file
    response.stream_to_file(speech_file_path)


    play_mp3(speech_file_path)


def play_mp3(path):
    pygame.mixer.init()  # Initialize the mixer module
    pygame.mixer.music.load(str(path))  # Load the MP3 file
    pygame.mixer.music.play()  # Play the MP3 file

    while pygame.mixer.music.get_busy():
        pygame.time.Clock().tick(10)

    pygame.mixer.music.stop()
    pygame.mixer.quit()

def generate_question():
    add_history("user", "give me a single interview question")
    st.chat_message("user").write("give me a single interview question")

    with st.chat_message("assistant"):
        chat_container = st.empty()

        with_message_history = RunnableWithMessageHistory(
            st.session_state["chain"],
            get_session_history,
            input_messages_key="input",
            history_messages_key="history",
        )

        # ìŠ¤íŠ¸ë¦¼ ì‘ë‹µì„ ì²˜ë¦¬í•˜ì—¬ ê²°ê³¼ë¥¼ ìƒì„±
        stream_response = with_message_history.invoke(
            {"input": "give me a single interview question"},
            config={"configurable": {"session_id": "abc123"}},
        )["output"]

        ai_answer = ""
        for chunk in stream_response:
            ai_answer += chunk
            if st.session_state.get("sound_button") == "Sound":
                chat_container.markdown("Sound ì˜µì…˜ì´ ì„ íƒë˜ì—ˆìŠµë‹ˆë‹¤.")
            else:
                chat_container.markdown(ai_answer)

        # AI ì‘ë‹µì„ ê¸°ë¡í•˜ê³ , í•„ìš” ì‹œ ì½ê¸° ê¸°ëŠ¥ ìˆ˜í–‰
        add_history("ai", stream_response)
        if st.session_state.get("sound_button") != "Text":
            read_outloud(stream_response)

 # ì„¸ì…˜ IDë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì„¸ì…˜ ê¸°ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
def get_session_history(session_ids: str) -> BaseChatMessageHistory:
    print(session_ids)
    if session_ids not in st.session_state["store"]:  # ì„¸ì…˜ IDê°€ storeì— ì—†ëŠ” ê²½ìš°
        # ìƒˆë¡œìš´ ChatMessageHistory ê°ì²´ë¥¼ ìƒì„±í•˜ì—¬ storeì— ì €ì¥
        st.session_state["store"][session_ids] = ChatMessageHistory()
    return st.session_state["store"][session_ids]  # í•´ë‹¹ ì„¸ì…˜ IDì— ëŒ€í•œ ì„¸ì…˜ ê¸°ë¡ ë°˜í™˜




def recognize_emotion(audio_file_path):
    # ê°ì • ì˜ˆì¸¡
    emotions = emotion_recognizer(audio_file_path)

    for emotion in emotions:
        label = emotion['label']
        score = emotion['score']
        print(f"Emotion: {label}, Confidence: {score:.2f}")

    return emotions[0]['label']






uploaded_file = st.file_uploader("Upload your resume (PDF format)", type="pdf")
if not uploaded_file:
    st.warning("Please upload a PDF file.")
else:
    with st.sidebar:
        st.image("./Logo.png",width=300)
        
        # ì¸í„°ë·° ìœ í˜• ë° ê¸°íƒ€ ì„¤ì • ì…ë ¥
        role_type = st.selectbox("Role", ["Software Engineer", "Data Scientist", "MLops Engineer"])
        interviewer_type = st.selectbox("interviewer", ["hiring manager", "technical lead"])
        company = st.text_input("Company", value="Meta")
        sound_button = st.selectbox("Text/Sound", ["Text", "Sound", "Both"])
        st.session_state["sound_button"] = sound_button

        submit_btn = st.button(" Save changes ")
        clear_btn = st.button("Clear chat history")

        # # ë²„íŠ¼ í…ìŠ¤íŠ¸ë¥¼ ì„¸ì…˜ ìƒíƒœë¡œ ì´ˆê¸°í™”
        # if "button_text" not in st.session_state:
        #     st.session_state["button_text"] = "Sound only"

        # # ë²„íŠ¼ í´ë¦­ ì´ë²¤íŠ¸ ì²˜ë¦¬
        # if st.button(st.session_state["button_text"]):
        #     # ë²„íŠ¼ í´ë¦­ ì‹œ í…ìŠ¤íŠ¸ ë³€ê²½
        #     if st.session_state["button_text"] == "Sound only":
        #         st.session_state["button_text"] = "Read only "
        #     else:
        #         st.session_state["button_text"] = "Sound only"

    stream_response = "This is a sample response text to be read out loud."


    if clear_btn:
        retriever = st.session_state["messages"].clear()

        
    if submit_btn:
        chain = modelCreation(role_type, interviewer_type, company, uploaded_file)
        del st.session_state["store"]   
        if chain:
            st.session_state["chain"] = chain
            print(f"Chain created {company}")

    if "chain" not in st.session_state:
        st.session_state["chain"] = modelCreation(role_type, interviewer_type, company, uploaded_file)


    print_history()

    # modelCreation í•¨ìˆ˜ ìˆ˜ì •ì€ ì´ì „ ì½”ë“œì—ì„œ ë™ì¼

   

    

    

    if user_input := st.chat_input():
        add_history("user", user_input)
        st.chat_message("user").write(user_input)
        with st.chat_message("assistant"):
            chat_container = st.empty()

            with_message_history = (
                RunnableWithMessageHistory(  # RunnableWithMessageHistory ê°ì²´ ìƒì„±
                    st.session_state["chain"],  # ì‹¤í–‰í•  Runnable ê°ì²´
                    get_session_history,  # ì„¸ì…˜ ê¸°ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
                    input_messages_key="input",  # ì…ë ¥ ë©”ì‹œì§€ì˜ í‚¤
                    history_messages_key="history",  # ê¸°ë¡ ë©”ì‹œì§€ì˜ í‚¤
                )
            )

            stream_response = with_message_history.invoke(
                {"input": "Answer of your question: " + user_input},
                config = {"configurable": {"session_id": "abc123"}},
                )["output"]
            ai_answer = ""
            for chunk in stream_response:
                ai_answer += chunk
                if st.session_state["sound_button"] == "Sound":
                    chat_container.markdown("Sound ì˜µì…˜ì´ ì„ íƒë˜ì—ˆìŠµë‹ˆë‹¤.")
                else:
                    chat_container.markdown(ai_answer)
                    
            add_history("ai", stream_response)
            if not st.session_state["sound_button"] == "Text":
                read_outloud(stream_response)


    if "chain" in st.session_state:

        # ë²„íŠ¼ í´ë¦­ ì‹œ ì‹¤í–‰ë˜ëŠ” í•¨ìˆ˜


        # í…ìŠ¤íŠ¸ì™€ ë²„íŠ¼ì„ ìœ„í•œ ë¹ˆ ì»¨í…Œì´ë„ˆë¥¼ ìƒì„±
        text_container = st.container()

        # # í…ìŠ¤íŠ¸ë¥¼ ë¨¼ì € ë Œë”ë§
        

        # ë²„íŠ¼ì„ í…ìŠ¤íŠ¸ ì•„ë˜ì—ì„œ ë Œë”ë§
        if st.button("Generate Interview Questions"):
            with text_container:
                generate_question()  # í…ìŠ¤íŠ¸ ì¶œë ¥ ë° ì§ˆë¬¸ ìƒì„±

        # `Record your response` ë²„íŠ¼ í´ë¦­ ì‹œ ë…¹ìŒ ë²„íŠ¼ í‘œì‹œ
        if st.button("Answer by recorded audio"):
                        
            audio_path = "./audio.wav"

            # Hugging Faceì˜ Whisper ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
            

            # í…ìŠ¤íŠ¸ ë³€í™˜
            transcription = transcriber(audio_path)["text"]
            emotion = recognize_emotion(audio_path)

            audioPrompt = f"{transcription} \n\n Emotion: {emotion}"
            
            add_history("user", transcription)
            st.chat_message("user").write(transcription)
            with st.chat_message("assistant"):
                chat_container = st.empty()

                with_message_history = (
                    RunnableWithMessageHistory(  # RunnableWithMessageHistory ê°ì²´ ìƒì„±
                        st.session_state["chain"],  # ì‹¤í–‰í•  Runnable ê°ì²´
                        get_session_history,  # ì„¸ì…˜ ê¸°ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
                        input_messages_key="input",  # ì…ë ¥ ë©”ì‹œì§€ì˜ í‚¤
                        history_messages_key="history",  # ê¸°ë¡ ë©”ì‹œì§€ì˜ í‚¤
                    )
                )

                stream_response = with_message_history.invoke(
                    {"input": "Answer of your question: " + audioPrompt},
                    config = {"configurable": {"session_id": "abc123"}},
                    )["output"]
                ai_answer = ""
                for chunk in stream_response:
                    ai_answer += chunk
                    if st.session_state["sound_button"] == "Sound":
                        chat_container.markdown("Sound ì˜µì…˜ì´ ì„ íƒë˜ì—ˆìŠµë‹ˆë‹¤.")
                    else:
                        chat_container.markdown(ai_answer)
                        
                add_history("ai", stream_response)
                if not st.session_state["sound_button"] == "Text":
                    read_outloud(stream_response)         
            

        
    